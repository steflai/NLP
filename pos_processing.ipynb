{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uR9Q2DX1zl2k"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import re\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "nlp = spacy.load('en_core_web_sm', parse=True, tag=True, entity=True)\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 292
    },
    "colab_type": "code",
    "id": "yiRjgTrHzs_q",
    "outputId": "d2411e1c-da6a-46ea-f3f9-fa1aba0c7d14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
      "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
      "You can now load the model via spacy.load('en')\n",
      "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "Collecting vaderSentiment\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/86/9e/c53e1fc61aac5ee490a6ac5e21b1ac04e55a7c2aba647bb8411c9aadf24e/vaderSentiment-3.2.1-py2.py3-none-any.whl (125kB)\n",
      "\u001b[K     |████████████████████████████████| 133kB 2.7MB/s \n",
      "\u001b[?25hInstalling collected packages: vaderSentiment\n",
      "Successfully installed vaderSentiment-3.2.1\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en\n",
    "!python -m spacy download en_core_web_sm\n",
    "!pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "35NlIEat1qSW"
   },
   "outputs": [],
   "source": [
    "toy_rev = pd.read_csv('Scrapped_Car_Reviews_Toyota.csv',engine='python',index_col=False)\n",
    "toy_rev['review']=toy_rev['Review_Title']+toy_rev['Review']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "k1ESU4slwvLC",
    "outputId": "091a4a62-7ab1-4d03-9960-2cbf93da6024"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22702, 8)"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_rev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "colab_type": "code",
    "id": "w-hLVKdt1ysy",
    "outputId": "e7277679-74d7-4651-a1f5-059b19ebfc29"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"df0852fd1c904f4da187c4e3ef0c0b76-0\" class=\"displacy\" width=\"1100\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Great</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">car</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">and</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">CCONJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">has</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">long</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">range</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-df0852fd1c904f4da187c4e3ef0c0b76-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-df0852fd1c904f4da187c4e3ef0c0b76-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-df0852fd1c904f4da187c4e3ef0c0b76-0-1\" stroke-width=\"2px\" d=\"M245,177.0 C245,89.5 395.0,89.5 395.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-df0852fd1c904f4da187c4e3ef0c0b76-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M395.0,179.0 L403.0,167.0 387.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-df0852fd1c904f4da187c4e3ef0c0b76-0-2\" stroke-width=\"2px\" d=\"M245,177.0 C245,2.0 575.0,2.0 575.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-df0852fd1c904f4da187c4e3ef0c0b76-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M575.0,179.0 L583.0,167.0 567.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-df0852fd1c904f4da187c4e3ef0c0b76-0-3\" stroke-width=\"2px\" d=\"M770,177.0 C770,89.5 920.0,89.5 920.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-df0852fd1c904f4da187c4e3ef0c0b76-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M770,179.0 L762,167.0 778,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-df0852fd1c904f4da187c4e3ef0c0b76-0-4\" stroke-width=\"2px\" d=\"M595,177.0 C595,2.0 925.0,2.0 925.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-df0852fd1c904f4da187c4e3ef0c0b76-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M925.0,179.0 L933.0,167.0 917.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "txt = 'Great car and has long range'\n",
    "doc = nlp(txt)\n",
    "spacy.displacy.render(doc,style='dep',jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 163
    },
    "colab_type": "code",
    "id": "IF3kz-aZ4reE",
    "outputId": "1a4cb0c2-b76d-4c2b-ceab-6a052db680ab"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-1535d52bd940>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'spacy.tokens.doc.Doc' object has no attribute 'dtype'"
     ]
    }
   ],
   "source": [
    "doc.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F83PtQzV16E6"
   },
   "outputs": [],
   "source": [
    "competitors = ['chevy', 'ford','nissan','honda','chevrolet','volkswagen','benz','mercedes','subaru','vw']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d2_ndGft6WPd"
   },
   "source": [
    "## identify competitors \n",
    "## Remove stopwords \n",
    "## classify pairs / parts of speech \n",
    "## Identify key words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "4--Y-D_DAFZQ",
    "outputId": "467c7039-8d21-4823-f33d-a709d32ea159"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now in march see when cherry blossoms are beginning to bloom the japanese leg of the olympic torch relay will kick off at a soccer stadium  miles from the fukushima daiichi nuclear power plant a nearby sports complex will also host the games baseball and softball matches thats bound to surprise and maybe worry some attendees whose memory of the power plants catastrophic meltdown nine years ago is still fresh but in fact the government has been aggressively decontaminating and rehabilitating fukushima prefecture and life is slowly returning to the exclusion zone\n"
     ]
    }
   ],
   "source": [
    "text = \"*Now In March, see when cherry blossoms are beginning to bloom, the Japanese leg of the Olympic Torch relay will kick off at a soccer stadium 12 miles from the Fukushima Daiichi Nuclear Power Plant. A nearby sports complex will also host the Games’ baseball and softball matches. That’s bound to surprise and maybe worry some attendees, whose memory of the power plant’s catastrophic meltdown nine years ago is still fresh. But in fact, the government has been aggressively decontaminating and rehabilitating Fukushima prefecture, and life is slowly returning to the exclusion zone.\"\n",
    "text = clean_text(text)\n",
    "print (text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zwUa5Rl_A8EP"
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    if text is not None:\n",
    "        data = str(text)\n",
    "        data =  data.lower()\n",
    "        data = re.sub('re:', '', data)\n",
    "        data = re.sub('-', '', data)\n",
    "        data = re.sub('_', '', data)\n",
    "        # Remove data between square brackets\n",
    "        data =re.sub('\\[[^]]*\\]', '', data)\n",
    "        # removes punctuation\n",
    "        data = re.sub(r'[^\\w\\s]','',data)\n",
    "        data = re.sub(r'\\n',' ',data)\n",
    "        data = re.sub(r'[0-9]+','',data)\n",
    "        # strip html \n",
    "        p = re.compile(r'<.*?>')\n",
    "        data = re.sub(r\"\\'ve\", \" have \", data)\n",
    "        data = re.sub(r\"can't\", \"cannot \", data)\n",
    "        data = re.sub(r\"n't\", \" not \", data)\n",
    "        data = re.sub(r\"I'm\", \"I am\", data)\n",
    "        data = re.sub(r\" m \", \" am \", data)\n",
    "        data = re.sub(r\"\\'re\", \" are \", data)\n",
    "        data = re.sub(r\"\\'d\", \" would \", data)\n",
    "        data = re.sub(r\"\\'ll\", \" will \", data)\n",
    "        return data\n",
    "    return 'No Subject'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YgObr5gJ6U2c"
   },
   "outputs": [],
   "source": [
    "def pos_logic (string): \n",
    "  amod_pairs = []\n",
    "  advmod_pairs = []\n",
    "  compound_pairs = []\n",
    "  xcomp_pairs = []\n",
    "  neg_pairs = []\n",
    "  doc = nlp(string) # apply spacy nlp, doc defined \n",
    "  str1=''\n",
    "  str2=''\n",
    "  for token in doc: \n",
    "    # below is logic to determine what parts of speech, iterate through a string \n",
    "      if token.pos_ is 'NOUN':\n",
    "        for j in token.lefts:\n",
    "          if j.dep_ == 'compound':\n",
    "            compound_pairs.append((j.text+' '+token.text,token.text))\n",
    "          if j.dep_ is 'amod' and j.pos_ is 'ADJ': #primary condition\n",
    "            str1 = j.text+' '+token.text\n",
    "            amod_pairs.append(j.text+' '+token.text)\n",
    "            for k in j.lefts:\n",
    "              if k.dep_ is 'advmod': #secondary condition to get adjective of adjectives\n",
    "                  str2 = k.text+' '+j.text+' '+token.text\n",
    "                  amod_pairs.append(k.text+' '+j.text+' '+token.text)\n",
    "              mtch = re.search(re.escape(str1),re.escape(str2))\n",
    "              if mtch is not None:\n",
    "                amod_pairs.remove(str1)\n",
    "        if token.pos_ is 'VERB':\n",
    "          for j in token.lefts:\n",
    "            if j.dep_ is 'advmod' and j.pos_ is 'ADV':\n",
    "              advmod_pairs.append(j.text+' '+token.text)\n",
    "            if j.dep_ is 'neg' and j.pos_ is 'ADV':\n",
    "              neg_pairs.append(j.text+' '+token.text)\n",
    "          for j in token.rights:\n",
    "            if j.dep_ is 'advmod'and j.pos_ is 'ADV':\n",
    "              advmod_pairs.append(token.text+' '+j.text)\n",
    "        if token.pos_ is 'ADJ':\n",
    "          for j,h in zip(token.rights,token.lefts):\n",
    "            if j.dep_ is 'xcomp' and h.dep_ is not 'neg':\n",
    "              for k in j.lefts:\n",
    "                if k.dep_ is 'aux':\n",
    "                  xcomp_pairs.append(token.text+' '+k.text+' '+j.text)\n",
    "            elif j.dep_ is 'xcomp' and h.dep_ is 'neg':\n",
    "              if k.dep_ is 'aux':\n",
    "                neg_pairs.append(h.text +' '+token.text+' '+k.text+' '+j.text)\n",
    "  return  amod_pairs, advmod_pairs, compound_pairs, xcomp_pairs, neg_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CD6HYwvcEWF7"
   },
   "outputs": [],
   "source": [
    "def pos_logic_comp (string): \n",
    "  eamod_pairs = []\n",
    "  eadvmod_pairs = []\n",
    "  ecompound_pairs = []\n",
    "  eneg_pairs = []\n",
    "  excomp_pairs = []\n",
    "  doc = nlp(string) #doc redefined here, these are the lines not caught by the logic gate. They are competitor reviews \n",
    "  str1=''\n",
    "  str2=''\n",
    "  for token in doc:\n",
    "    if token.pos_ is 'NOUN':\n",
    "      for j in token.lefts:\n",
    "        if j.dep_ == 'compound':\n",
    "          ecompound_pairs.append((j.text+' '+token.text,token.text))\n",
    "        if j.dep_ is 'amod' and j.pos_ is 'ADJ': #primary condition\n",
    "          str1 = j.text+' '+token.text\n",
    "          eamod_pairs.append(j.text+' '+token.text)\n",
    "          for k in j.lefts:\n",
    "            if k.dep_ is 'advmod': #secondary condition to get adjective of adjectives\n",
    "              str2 = k.text+' '+j.text+' '+token.text\n",
    "              eamod_pairs.append(k.text+' '+j.text+' '+token.text)\n",
    "          mtch = re.search(re.escape(str1),re.escape(str2))\n",
    "          if mtch is not None:\n",
    "            eamod_pairs.remove(str1) #appears to be same as above for NOUNS\n",
    "    if token.pos_ is 'VERB':\n",
    "      for j in token.lefts:\n",
    "        if j.dep_ is 'advmod' and j.pos_ is 'ADV':\n",
    "          eadvmod_pairs.append(j.text+' '+token.text)\n",
    "        if j.dep_ is 'neg' and j.pos_ is 'ADV':\n",
    "          eneg_pairs.append(j.text+' '+token.text)\n",
    "      for j in token.rights:\n",
    "        if j.dep_ is 'advmod'and j.pos_ is 'ADV':\n",
    "          eadvmod_pairs.append(token.text+' '+j.text) #appears to be the same for VERBS\n",
    "    if token.pos_ is 'ADJ':\n",
    "      for j in token.rights:\n",
    "        if j.dep_ is 'xcomp':\n",
    "          for k in j.lefts:\n",
    "            if k.dep_ is 'aux':\n",
    "              excomp_pairs.append(token.text+' '+k.text+' '+j.text) #differnt for adj, no accounting for 'negs' here?? \n",
    "  return eamod_pairs, eadvmod_pairs, ecompound_pairs, eneg_pairs, excomp_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 836
    },
    "colab_type": "code",
    "id": "M7i7aQ0OKS09",
    "outputId": "103d570e-438b-48ac-84dd-c6524617a3d3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22702/22702 [07:43<00:00, 48.99it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Review_Date</th>\n",
       "      <th>Author_Name</th>\n",
       "      <th>Vehicle_Title</th>\n",
       "      <th>Review_Title</th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "      <th>review</th>\n",
       "      <th>compound_nouns</th>\n",
       "      <th>aspect_keywords</th>\n",
       "      <th>competition_comp_nouns</th>\n",
       "      <th>competition_aspects</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>on 02/02/17 19:53 PM (PST)</td>\n",
       "      <td>Ricardo</td>\n",
       "      <td>1997 Toyota Previa Minivan LE 3dr Minivan</td>\n",
       "      <td>great vehicle, Toyota best design ever. thank you</td>\n",
       "      <td>there is no way back, enjoy what you have .</td>\n",
       "      <td>5.000</td>\n",
       "      <td>great vehicle, Toyota best design ever. thank ...</td>\n",
       "      <td>[(vehicle toyota, toyota)]</td>\n",
       "      <td>[great vehicle toyota, best design]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>on 12/17/16 16:40 PM (PST)</td>\n",
       "      <td>matt</td>\n",
       "      <td>1997 Toyota Previa Minivan LE All-Trac 3dr Min...</td>\n",
       "      <td>my 4th previa, best van ever made!</td>\n",
       "      <td>1st 95 went over 300k before being totalled b...</td>\n",
       "      <td>5.000</td>\n",
       "      <td>my 4th previa, best van ever made! 1st 95 went...</td>\n",
       "      <td>[(captain chairs, chairs), (craigslist talk, t...</td>\n",
       "      <td>[minor quirks, mini rv loads, middle bench, ro...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>on 04/14/10 07:43 AM (PDT)</td>\n",
       "      <td>Joel G</td>\n",
       "      <td>1997 Toyota Previa Minivan LE 3dr Minivan</td>\n",
       "      <td>Mom's Taxi Babies Ride</td>\n",
       "      <td>Sold 86 Toyota Van 285K miles to be replaced ...</td>\n",
       "      <td>5.000</td>\n",
       "      <td>Mom's Taxi Babies Ride Sold 86 Toyota Van 285K...</td>\n",
       "      <td>[(moms babies, babies), (taxi babies, babies),...</td>\n",
       "      <td>[remote bat, younger brothers, middle seat, ap...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>on 11/12/08 17:31 PM (PST)</td>\n",
       "      <td>Dennis</td>\n",
       "      <td>1997 Toyota Previa Minivan LE All-Trac 3dr Min...</td>\n",
       "      <td>My Favorite Van Ever</td>\n",
       "      <td>I have owned lots of vans, and the Previa is ...</td>\n",
       "      <td>4.875</td>\n",
       "      <td>My Favorite Van Ever I have owned lots of vans...</td>\n",
       "      <td>[(fuel mileage, mileage), (toyota salesman, sa...</td>\n",
       "      <td>[toyota handling, mid engine, stupid handling,...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>on 04/14/08 22:47 PM (PDT)</td>\n",
       "      <td>Alf Skrastins</td>\n",
       "      <td>1997 Toyota Previa Minivan LE All-Trac 3dr Min...</td>\n",
       "      <td>Best Minivan ever</td>\n",
       "      <td>My 1997 AWD Previa is the third one that I ha...</td>\n",
       "      <td>5.000</td>\n",
       "      <td>Best Minivan ever My 1997 AWD Previa is the th...</td>\n",
       "      <td>[(k mi, mi), (gas mileage, mileage)]</td>\n",
       "      <td>[reasonable replacement, awd previa, third one...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0  ... competition_aspects\n",
       "0          0  ...                  []\n",
       "1          1  ...                  []\n",
       "2          2  ...                  []\n",
       "3          3  ...                  []\n",
       "4          4  ...                  []\n",
       "\n",
       "[5 rows x 12 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "easpect_terms = []\n",
    "ecomp_terms = []\n",
    "aspect_terms = []\n",
    "comp_terms= []\n",
    "competitors_mentioned = []\n",
    "\n",
    "for x in tqdm(range(len(toy_rev['review']))):\n",
    "  counter = 0\n",
    "  if toy_rev['review'][x] is not \"Nan\": \n",
    "    text = toy_rev['review'][x]\n",
    "    cleaned_lines = clean_text(text)\n",
    "  for comp in competitors: \n",
    "    if comp in cleaned_lines: \n",
    "        counter = counter +1\n",
    "        competitors_mentioned.append(comp) \n",
    "  if counter >0: \n",
    "    eamod_pairs, eadvmod_pairs, ecompound_pairs, eneg_pairs, excomp_pairs = pos_logic_comp(cleaned_lines)\n",
    "    epairs = list(set(eamod_pairs+ eadvmod_pairs+ eneg_pairs ))   \n",
    "    for i in range(len(epairs)):\n",
    "            if len(ecompound_pairs)!=0:\n",
    "                for comp in ecompound_pairs:\n",
    "                    mtch = re.search(re.escape(comp[1]),re.escape(epairs[i]))\n",
    "                    if mtch is not None:\n",
    "                        epairs[i] = epairs[i].replace(mtch.group(),comp[0])\n",
    "    easpect_terms.append(epairs)\n",
    "    ecomp_terms.append(ecompound_pairs)\n",
    "    #ecomp_terms.append (excomp_pairs)\n",
    "    aspect_terms.append([])\n",
    "    comp_terms.append([])\n",
    "  else: \n",
    "    amod_pairs, advmod_pairs, compound_pairs, xcomp_pairs, neg_pairs = pos_logic(cleaned_lines)\n",
    "    pairs = list(set(amod_pairs+ advmod_pairs+ neg_pairs))\n",
    "    for i in range(len(pairs)):\n",
    "            if len(compound_pairs)!=0:\n",
    "                for comp in compound_pairs:\n",
    "                    mtch = re.search(re.escape(comp[1]),re.escape(pairs[i]))\n",
    "                    if mtch is not None:\n",
    "                        pairs[i] = pairs[i].replace(mtch.group(),comp[0])    \n",
    "    aspect_terms.append(pairs)\n",
    "    comp_terms.append(compound_pairs)\n",
    "    #comp_terms.append(xcomp_pairs)\n",
    "    easpect_terms.append([])\n",
    "    ecomp_terms.append([])\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "toy_rev['compound_nouns'] = comp_terms\n",
    "toy_rev['aspect_keywords'] = aspect_terms\n",
    "toy_rev['competition_comp_nouns'] = ecomp_terms\n",
    "toy_rev['competition_aspects'] = easpect_terms\n",
    "toy_rev.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "cX7vFDkjAYig",
    "outputId": "fb93eae3-8833-4072-dd82-0c47f1364626"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22702\n",
      "22702\n",
      "22702\n",
      "22702\n"
     ]
    }
   ],
   "source": [
    "print (len(comp_terms))\n",
    "print (len(aspect_terms))\n",
    "print (len(ecomp_terms))\n",
    "print (len(easpect_terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vmlvRwaHNE2u"
   },
   "outputs": [],
   "source": [
    "toy_rev.to_csv('toy_rev_procsed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hZOTYSQT0LdQ"
   },
   "outputs": [],
   "source": [
    "# Reviewing logic of substituting compound pairs into the pairs list to add context. Just keeping notes here\n",
    "\n",
    "amod_pairs, advmod_pairs, compound_pairs, xcomp_pairs, neg_pairs = pos_logic(toy_rev['review'][100])\n",
    "pairs = list(set(amod_pairs+ advmod_pairs+ neg_pairs))\n",
    "#print (compound_pairs[0])\n",
    "#print (compound_pairs[0][1])\n",
    "#print (pairs[0])\n",
    "\n",
    "# logic: if there is a match\n",
    "# the second word in a compound pair is the same as \n",
    "# for each word in compound pairs, \n",
    "# a match equals if the second word equals any word in the list of pairs \n",
    "# if there is a match, replace the words in the paris list with the compound pair \n",
    "\n",
    "# had to remove xcomp and compound pairs for pairs list \n",
    "\n",
    "# switch around logic in main code -- makes more sense to check if there are compound pairs before running loop??\n",
    "for i in range(len(pairs)):\n",
    "    if len(compound_pairs)!=0:\n",
    "                for word in compound_pairs:\n",
    "                    mtch = re.search(re.escape(word[1]),re.escape(pairs[i]))\n",
    "                    if mtch is not None:\n",
    "                        pairs[i] = pairs[i].replace(mtch.group(),comp[0]) \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-e5GsJ10SlXu"
   },
   "source": [
    "For sentiment analysis portion. The original notebook runs Vader on just the aspect terms. Against the table's own ratings, the classifier has 78% accuracy. \n",
    "\n",
    "\n",
    "I will attempt to train an analysis model on a separate script. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# See analysis Multinomial NB model in separate file \n",
    "- can use pickle to save and import to here \n",
    "- however the NB models uses vectors. so all the preprocessing done here isn't useful \n",
    "- stick to using vadar for this as an option because tbh the accuracy is similar - 75-78pc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "pos_processing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
