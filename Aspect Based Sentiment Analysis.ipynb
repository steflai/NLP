{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] The system cannot find the file specified: '../input'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-b8562ee66662>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../input'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified: '../input'"
     ]
    }
   ],
   "source": [
    "os.chdir('../input')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "source": [
    "# Aspect Based Sentiment Analysis on Car Reviews\n",
    "## Taking Toyota Cars as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Review_Date</th>\n",
       "      <th>Author_Name</th>\n",
       "      <th>Vehicle_Title</th>\n",
       "      <th>Review_Title</th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>on 02/02/17 19:53 PM (PST)</td>\n",
       "      <td>Ricardo</td>\n",
       "      <td>1997 Toyota Previa Minivan LE 3dr Minivan</td>\n",
       "      <td>great vehicle, Toyota best design ever. thank you</td>\n",
       "      <td>there is no way back, enjoy what you have .</td>\n",
       "      <td>5.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>on 12/17/16 16:40 PM (PST)</td>\n",
       "      <td>matt</td>\n",
       "      <td>1997 Toyota Previa Minivan LE All-Trac 3dr Min...</td>\n",
       "      <td>my 4th previa, best van ever made!</td>\n",
       "      <td>1st 95 went over 300k before being totalled b...</td>\n",
       "      <td>5.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>on 04/14/10 07:43 AM (PDT)</td>\n",
       "      <td>Joel G</td>\n",
       "      <td>1997 Toyota Previa Minivan LE 3dr Minivan</td>\n",
       "      <td>Mom's Taxi Babies Ride</td>\n",
       "      <td>Sold 86 Toyota Van 285K miles to be replaced ...</td>\n",
       "      <td>5.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>on 11/12/08 17:31 PM (PST)</td>\n",
       "      <td>Dennis</td>\n",
       "      <td>1997 Toyota Previa Minivan LE All-Trac 3dr Min...</td>\n",
       "      <td>My Favorite Van Ever</td>\n",
       "      <td>I have owned lots of vans, and the Previa is ...</td>\n",
       "      <td>4.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>on 04/14/08 22:47 PM (PDT)</td>\n",
       "      <td>Alf Skrastins</td>\n",
       "      <td>1997 Toyota Previa Minivan LE All-Trac 3dr Min...</td>\n",
       "      <td>Best Minivan ever</td>\n",
       "      <td>My 1997 AWD Previa is the third one that I ha...</td>\n",
       "      <td>5.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0                  Review_Date     Author_Name  \\\n",
       "0          0   on 02/02/17 19:53 PM (PST)        Ricardo    \n",
       "1          1   on 12/17/16 16:40 PM (PST)           matt    \n",
       "2          2   on 04/14/10 07:43 AM (PDT)         Joel G    \n",
       "3          3   on 11/12/08 17:31 PM (PST)         Dennis    \n",
       "4          4   on 04/14/08 22:47 PM (PDT)  Alf Skrastins    \n",
       "\n",
       "                                       Vehicle_Title  \\\n",
       "0          1997 Toyota Previa Minivan LE 3dr Minivan   \n",
       "1  1997 Toyota Previa Minivan LE All-Trac 3dr Min...   \n",
       "2          1997 Toyota Previa Minivan LE 3dr Minivan   \n",
       "3  1997 Toyota Previa Minivan LE All-Trac 3dr Min...   \n",
       "4  1997 Toyota Previa Minivan LE All-Trac 3dr Min...   \n",
       "\n",
       "                                        Review_Title  \\\n",
       "0  great vehicle, Toyota best design ever. thank you   \n",
       "1                 my 4th previa, best van ever made!   \n",
       "2                             Mom's Taxi Babies Ride   \n",
       "3                               My Favorite Van Ever   \n",
       "4                                  Best Minivan ever   \n",
       "\n",
       "                                              Review  Rating  \n",
       "0        there is no way back, enjoy what you have .   5.000  \n",
       "1   1st 95 went over 300k before being totalled b...   5.000  \n",
       "2   Sold 86 Toyota Van 285K miles to be replaced ...   5.000  \n",
       "3   I have owned lots of vans, and the Previa is ...   4.875  \n",
       "4   My 1997 AWD Previa is the third one that I ha...   5.000  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_rev = pd.read_csv('Scrapped_Car_Reviews_Toyota.csv',engine='python',index_col=False)\n",
    "toy_rev.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Combining the review title and review body for the text corpus****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_rev['review']=toy_rev['Review_Title']+toy_rev['Review']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Using spaCy for dependency parsing which forms the crux of aspect extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-11e57a205ea4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'en_core_web_lg'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from tqdm import tqdm\n",
    "nlp = spacy.load('en_core_web_lg', parse=True, tag=True, entity=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## **Using spaCy's awesome displacy module to show the dependency relations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = 'Great car and has long range'\n",
    "doc = nlp(txt)\n",
    "spacy.displacy.render(doc,style='dep',jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**from https://nlp.stanford.edu/software/dependencies_manual.pdf**\n",
    "### AMOD - adjectival modifier\n",
    "#### An adjectival modifier of a Noun is any adjectival phrase that serves to modify the meaning of the Noun\n",
    "### ex - 'Great <--amod-- Car', 'Long <--amod-- range'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = 'Drives well has great handling'\n",
    "doc = nlp(txt)\n",
    "spacy.displacy.render(doc,style='dep',jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADVMOD - adverb modifier\n",
    "#### An adverb modifier of a word is a (non-clausal) adverb or adverb-headed phrase that serves to modify\n",
    "#### the meaning of the word\n",
    "### ex - 'Drives --advmod--> well'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt =  \"wonderful to drive the camry \"\n",
    "doc = nlp(txt)\n",
    "spacy.displacy.render(doc,style='dep',jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XCOMP -  open clausal complement\n",
    "#### An open clausal complement (xcomp) of a verb or an adjective is a predicative or clausal complement without its own subject\n",
    "### ex - 'wonderful --xcomp--> drive'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt =  \"not wonderful to drive the camry \"\n",
    "doc = nlp(txt)\n",
    "spacy.displacy.render(doc,style='dep',jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NEG - self explanatory\n",
    "### ex - not <--neg-- wonderful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COMPOUND WORDS\n",
    "#### Generally from a review standpoint, compound words often do not offer us sentiments per se, hence my code looks for possible compound word pairs and then checks with the aspect words extracted if it can add more detail to the extracted aspects - ex Outstanding passenger van gives *more context* than Outstanding van (which is what my code would have extracted without the compound word search) while the compound word search will identify passenger van as a compound word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "competitors = ['Chevy','chevy','Ford','ford','Nissan','nissan','Honda','honda','Chevrolet','chevrolet','Volkswagen','volkswagen','benz','Benz','Mercedes','mercedes','subaru','Subaru','VW']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reason for using competitor name list is to remove potential misleading aspects-sentiments, since we are interested to acquire aspect info about Toyota and not any other brand. This is because a reviewer might be comparing a Benz saying it has superior handling when compared to the car the person is reviewing and this can lead to misclassifications******"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspect_terms = []\n",
    "comp_terms = []\n",
    "easpect_terms = []\n",
    "ecomp_terms = []\n",
    "enemy = []\n",
    "for x in tqdm(range(len(toy_rev['review']))):\n",
    "    amod_pairs = []\n",
    "    advmod_pairs = []\n",
    "    compound_pairs = []\n",
    "    xcomp_pairs = []\n",
    "    neg_pairs = []\n",
    "    eamod_pairs = []\n",
    "    eadvmod_pairs = []\n",
    "    ecompound_pairs = []\n",
    "    eneg_pairs = []\n",
    "    excomp_pairs = []\n",
    "    enemlist = []\n",
    "    if len(str(toy_rev['review'][x])) != 0:\n",
    "        lines = str(toy_rev['review'][x]).replace('*',' ').replace('-',' ').replace('so ',' ').replace('be ',' ').replace('are ',' ').replace('just ',' ').replace('get ','').replace('were ',' ').replace('When ','').replace('when ','').replace('again ',' ').replace('where ','').replace('how ',' ').replace('has ',' ').replace('Here ',' ').replace('here ',' ').replace('now ',' ').replace('see ',' ').replace('why ',' ').split('.')       \n",
    "        for line in lines:\n",
    "            enem_list = []\n",
    "            for eny in competitors:\n",
    "                enem = re.search(eny,line)\n",
    "                if enem is not None:\n",
    "                    enem_list.append(enem.group())\n",
    "            if len(enem_list)==0:\n",
    "                doc = nlp(line)\n",
    "                str1=''\n",
    "                str2=''\n",
    "                for token in doc:\n",
    "                    if token.pos_ is 'NOUN':\n",
    "                        for j in token.lefts:\n",
    "                            if j.dep_ == 'compound':\n",
    "                                compound_pairs.append((j.text+' '+token.text,token.text))\n",
    "                            if j.dep_ is 'amod' and j.pos_ is 'ADJ': #primary condition\n",
    "                                str1 = j.text+' '+token.text\n",
    "                                amod_pairs.append(j.text+' '+token.text)\n",
    "                                for k in j.lefts:\n",
    "                                    if k.dep_ is 'advmod': #secondary condition to get adjective of adjectives\n",
    "                                        str2 = k.text+' '+j.text+' '+token.text\n",
    "                                        amod_pairs.append(k.text+' '+j.text+' '+token.text)\n",
    "                                mtch = re.search(re.escape(str1),re.escape(str2))\n",
    "                                if mtch is not None:\n",
    "                                    amod_pairs.remove(str1)\n",
    "                    if token.pos_ is 'VERB':\n",
    "                        for j in token.lefts:\n",
    "                            if j.dep_ is 'advmod' and j.pos_ is 'ADV':\n",
    "                                advmod_pairs.append(j.text+' '+token.text)\n",
    "                            if j.dep_ is 'neg' and j.pos_ is 'ADV':\n",
    "                                neg_pairs.append(j.text+' '+token.text)\n",
    "                        for j in token.rights:\n",
    "                            if j.dep_ is 'advmod'and j.pos_ is 'ADV':\n",
    "                                advmod_pairs.append(token.text+' '+j.text)\n",
    "                    if token.pos_ is 'ADJ':\n",
    "                        for j,h in zip(token.rights,token.lefts):\n",
    "                            if j.dep_ is 'xcomp' and h.dep_ is not 'neg':\n",
    "                                for k in j.lefts:\n",
    "                                    if k.dep_ is 'aux':\n",
    "                                        xcomp_pairs.append(token.text+' '+k.text+' '+j.text)\n",
    "                            elif j.dep_ is 'xcomp' and h.dep_ is 'neg':\n",
    "                                if k.dep_ is 'aux':\n",
    "                                        neg_pairs.append(h.text +' '+token.text+' '+k.text+' '+j.text)\n",
    "            \n",
    "            else:\n",
    "                enemlist.append(enem_list)\n",
    "                doc = nlp(line)\n",
    "                str1=''\n",
    "                str2=''\n",
    "                for token in doc:\n",
    "                    if token.pos_ is 'NOUN':\n",
    "                        for j in token.lefts:\n",
    "                            if j.dep_ == 'compound':\n",
    "                                ecompound_pairs.append((j.text+' '+token.text,token.text))\n",
    "                            if j.dep_ is 'amod' and j.pos_ is 'ADJ': #primary condition\n",
    "                                str1 = j.text+' '+token.text\n",
    "                                eamod_pairs.append(j.text+' '+token.text)\n",
    "                                for k in j.lefts:\n",
    "                                    if k.dep_ is 'advmod': #secondary condition to get adjective of adjectives\n",
    "                                        str2 = k.text+' '+j.text+' '+token.text\n",
    "                                        eamod_pairs.append(k.text+' '+j.text+' '+token.text)\n",
    "                                mtch = re.search(re.escape(str1),re.escape(str2))\n",
    "                                if mtch is not None:\n",
    "                                    eamod_pairs.remove(str1)\n",
    "                    if token.pos_ is 'VERB':\n",
    "                        for j in token.lefts:\n",
    "                            if j.dep_ is 'advmod' and j.pos_ is 'ADV':\n",
    "                                eadvmod_pairs.append(j.text+' '+token.text)\n",
    "                            if j.dep_ is 'neg' and j.pos_ is 'ADV':\n",
    "                                eneg_pairs.append(j.text+' '+token.text)\n",
    "                        for j in token.rights:\n",
    "                            if j.dep_ is 'advmod'and j.pos_ is 'ADV':\n",
    "                                eadvmod_pairs.append(token.text+' '+j.text)\n",
    "                    if token.pos_ is 'ADJ':\n",
    "                        for j in token.rights:\n",
    "                            if j.dep_ is 'xcomp':\n",
    "                                for k in j.lefts:\n",
    "                                    if k.dep_ is 'aux':\n",
    "                                        excomp_pairs.append(token.text+' '+k.text+' '+j.text)\n",
    "        pairs = list(set(amod_pairs+advmod_pairs+neg_pairs+xcomp_pairs))\n",
    "        epairs = list(set(eamod_pairs+eadvmod_pairs+eneg_pairs+excomp_pairs))\n",
    "        for i in range(len(pairs)): #replace with compound word pairs \n",
    "            if len(compound_pairs)!=0:\n",
    "                for comp in compound_pairs:\n",
    "                    mtch = re.search(re.escape(comp[1]),re.escape(pairs[i]))\n",
    "                    if mtch is not None:\n",
    "                        pairs[i] = pairs[i].replace(mtch.group(),comp[0])\n",
    "        for i in range(len(epairs)):\n",
    "            if len(ecompound_pairs)!=0:\n",
    "                for comp in ecompound_pairs:\n",
    "                    mtch = re.search(re.escape(comp[1]),re.escape(epairs[i]))\n",
    "                    if mtch is not None:\n",
    "                        epairs[i] = epairs[i].replace(mtch.group(),comp[0])\n",
    "            \n",
    "    aspect_terms.append(pairs)\n",
    "    comp_terms.append(compound_pairs)\n",
    "    easpect_terms.append(epairs)\n",
    "    ecomp_terms.append(ecompound_pairs)\n",
    "    enemy.append(enemlist)\n",
    "toy_rev['compound_nouns'] = comp_terms\n",
    "toy_rev['aspect_keywords'] = aspect_terms\n",
    "toy_rev['competition'] = enemy\n",
    "toy_rev['competition_comp_nouns'] = ecomp_terms\n",
    "toy_rev['competition_aspects'] = easpect_terms\n",
    "toy_rev.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We use vaderSentiment for sentiment analysis because of it's speed and simplicity. It offers 3 types of polarity -  positive, negative and neutral. As a result we can filter all aspects which have high neutral scores hence minimizing errors caused due to wrong extraction of aspects and stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyser = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "sentiment = []\n",
    "for i in range(len(toy_rev)):\n",
    "    score_dict={'pos':0,'neg':0,'neu':0}\n",
    "    if len(toy_rev['aspect_keywords'][i])!=0: \n",
    "        for aspects in toy_rev['aspect_keywords'][i]:\n",
    "            sent = analyser.polarity_scores(aspects)\n",
    "            score_dict['neg'] += sent['neg']\n",
    "            score_dict['pos'] += sent['pos']\n",
    "        #score_dict['neu'] += sent['neu']\n",
    "        sentiment.append(max(score_dict.items(), key=operator.itemgetter(1))[0])\n",
    "    else:\n",
    "        sentiment.append('NaN')\n",
    "toy_rev['sentiment'] = sentiment\n",
    "toy_rev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_sent = []\n",
    "for sent in toy_rev['sentiment']:\n",
    "    if sent is 'NaN':\n",
    "        int_sent.append('NaN')\n",
    "    elif sent is 'pos':\n",
    "        int_sent.append('1')\n",
    "    else:\n",
    "        int_sent.append('0')\n",
    "toy_rev['int_sent'] = int_sent\n",
    "toy_rev.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we have arbitarily taken ratings greater than 3 as positive and everything else as negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "pos = []\n",
    "for i in range(len(toy_rev)):\n",
    "    if not math.isnan(toy_rev['Rating'][i]):\n",
    "        if int(toy_rev['Rating'][i])>3:\n",
    "            pos.append('1')\n",
    "        else:\n",
    "            pos.append('0')\n",
    "    else:\n",
    "        pos.append('0')\n",
    "toy_rev['Positive Review'] = pos\n",
    "toy_rev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'sent':toy_rev['Positive Review'],'sent_pred':toy_rev['int_sent']}\n",
    "metric_df = pd.DataFrame(data=d)\n",
    "metric_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(metric_df.sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing NaN values in the sentiment predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df = metric_df[metric_df.sent_pred != 'NaN']\n",
    "len(metric_df.sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,auc,f1_score,recall_score,precision_score\n",
    "print('accuracy')\n",
    "print(accuracy_score(metric_df.sent, metric_df.sent_pred))\n",
    "print('f1 score')\n",
    "print(f1_score(metric_df.sent, metric_df.sent_pred,pos_label='1'))\n",
    "print('recall')\n",
    "print(recall_score(metric_df.sent, metric_df.sent_pred,pos_label='1'))\n",
    "print('precision')\n",
    "print(precision_score(metric_df.sent, metric_df.sent_pred,pos_label='1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible improvements that can be made\n",
    "*  Tricky situation of removing stopwords to reduce unwanted extractions of non-aspects but this can also affect spaCy's dependency parsing. Same goes with noun chunk merging as well. If someone can think of a better way to remove stopwords and still retain spaCy's dependency goodness it can greatly improve the accuracy\n",
    "\n",
    "* This is not a ML task per se since we do more of parsing than ML. Although Bi-Directional LSTM have been very good at ABSA tasks in the past, unlike semeval tasks we do not have a fixed topic for our aspects to fall into. If someone can use the parsing aspect of the code to implement BLSTM in this case, that would be great\n",
    "\n",
    "* better alternatives to vaderSentiment if available (unsupervised/ semi-supervised methods might be better here I think)\n",
    "\n",
    "* The very definition of aspects can be a bit vague at times hence we do not have a valid metric to measure the aspect extraction's accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P.S This is my first Kaggle Kernel and I am fairly new to python programming as well, hence my non usage of list comprehensions and functions might be evident. I highly encourage everyone to fork my code and add your own twists to increase the accuracy of both aspect extractions and sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
