{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\stefl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "import nltk; nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('Claude.txt','r')\n",
    "claude = file.read()\n",
    "file.close()\n",
    "file = open(\"Ida.txt\", 'r')\n",
    "ida = file.read()\n",
    "file.close()\n",
    "file = open ('Youyou_Claire.txt', 'r')\n",
    "yc = file.read()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess (data):\n",
    "    if data is not None:\n",
    "        #exclusions = ['RE:', 'Re:', 're:']\n",
    "        #exclusions = '|'.join(exclusions)\n",
    "        data =  data.lower()\n",
    "        data = re.sub('re:', '', data)\n",
    "        data = re.sub('-', '', data)\n",
    "        data = re.sub('_', '', data)\n",
    "        # Remove data between square brackets\n",
    "        data =re.sub('\\[[^]]*\\]', '', data)\n",
    "        # removes punctuation\n",
    "        data = re.sub(r'[^\\w\\s]','',data)\n",
    "        data = re.sub(r'\\n',' ',data)\n",
    "        data = re.sub(r'[0-9]+','',data)\n",
    "        # strip html \n",
    "        p = re.compile(r'<.*?>')\n",
    "        data = re.sub(r\"\\'ve\", \" have \", data)\n",
    "        data = re.sub(r\"can't\", \"cannot \", data)\n",
    "        data = re.sub(r\"n't\", \" not \", data)\n",
    "        data = re.sub(r\"I'm\", \"I am\", data)\n",
    "        data = re.sub(r\" m \", \" am \", data)\n",
    "        data = re.sub(r\"\\'re\", \" are \", data)\n",
    "        data = re.sub(r\"\\'d\", \" would \", data)\n",
    "        data = re.sub(r\"\\'ll\", \" will \", data)\n",
    "        return data\n",
    "    return 'No Subject'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = preprocess(claude)\n",
    "doc2 = preprocess(ida)\n",
    "doc3 = preprocess(yc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [doc1, doc2, doc3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocess runs on strings so run first then put into list. if in dataframe use .apply\n",
    "\n",
    "Next creat term-doc matrix with cv.fit_transform. it also removes stopwords) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words = stop_words + ['um', 'im', 'yeah'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(max_df = 0.85, stop_words = stop_words, ngram_range=(1,3), max_features = 50)\n",
    "# min_df takes out words occuring in less than x documents or x proportion of documents, max_df for word that are too common - occuring in more than x number or proportion of documents\n",
    "# ngram_range takes min and max ie. (1,2) means unigrams and bigrams. use (1,3) for uni to trigrams\n",
    "# max_features limits to number of vocab returned \n",
    "\n",
    "word_count_vector = cv.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv.stop_words_ checks stop words\n",
    "# cv.vocabulary_ to see what we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc1</th>\n",
       "      <th>doc2</th>\n",
       "      <th>doc3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>american</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>anymore</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>back home</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>card</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>china</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>chinese</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>citizenship</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>coffee</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>countries</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>couple</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>difficult</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>easy</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>everyone</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>experience</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>feels</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>finding</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>friends</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>fucking</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>go back</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>green</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>green card</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>hes</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>hong</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>hong kong</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>internship</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>journalism</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>know mean</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>kong</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>life</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>like theres</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>little</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>made</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>money</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>new york city</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>pay</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>read</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>school</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>sometimes</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>soon</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>stay</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>stories</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>story</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>students</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>stuff like</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>super</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>talk</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>train</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>understand</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>us</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>york city</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               doc1  doc2  doc3\n",
       "american          1     0    15\n",
       "anymore           2     5     0\n",
       "back home         0     1    10\n",
       "card              0     0    12\n",
       "china             0     0    52\n",
       "chinese           1     0    21\n",
       "citizenship       0     0    11\n",
       "coffee            0     5     3\n",
       "countries         1     0     8\n",
       "couple            0     8     2\n",
       "difficult         8     1     0\n",
       "easy              3     0     6\n",
       "everyone          4     8     0\n",
       "experience        0     2     9\n",
       "feels             3     0     6\n",
       "finding           2     5     0\n",
       "friends           0     8    15\n",
       "fucking           0     8     0\n",
       "go back           4     0    13\n",
       "green             0     1    13\n",
       "green card        0     0    12\n",
       "hes               5     0     8\n",
       "hong              6     0     3\n",
       "hong kong         6     0     3\n",
       "internship        0     0     7\n",
       "journalism        0     0    12\n",
       "know mean         6     2     0\n",
       "kong              6     0     3\n",
       "life              0    10     7\n",
       "like theres       0     6     5\n",
       "little            0     6     9\n",
       "made              6     0     2\n",
       "money             0     2     8\n",
       "new york city     0     0     8\n",
       "pay               0     2     5\n",
       "read              0     3     7\n",
       "school            0     0    18\n",
       "sometimes         0    12     7\n",
       "soon              0     6     2\n",
       "stay              1     0    27\n",
       "stories           5     0     7\n",
       "story             0     1    10\n",
       "students          0     0     8\n",
       "stuff like        2     8     0\n",
       "super             1     0     7\n",
       "talk              0    10     5\n",
       "train             6     2     0\n",
       "understand        0     3     6\n",
       "us                1     0     9\n",
       "york city         0     0     8"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(word_count_vector.T.todense(),index = cv.get_feature_names(), columns=['doc1', 'doc2', 'doc3'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 50)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count_vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, compute IDF values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train the transfomer \n",
    "\n",
    "transformer = TfidfTransformer(smooth_idf=True, use_idf=True)\n",
    "transformer.fit(word_count_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: An extremely important point to note here is that the IDF should always be based on a large corpora and should be representative of texts you would be using to extract keywords. This is why we are using texts from 20,000 stack overflow posts to compute the IDF instead of just a handful. I’ve seen several articles on the Web that compute the IDF using a handful of documents. You will defeat the whole purpose of IDF weighting if its not based on a large corpora as (a) your vocabulary becomes too small and (b) you have limited ability to observe the behavior of words that you do know about.\n",
    "\n",
    "\n",
    "!!!\n",
    "Would be ideal to have historic dataset of interviews\n",
    "\n",
    "\n",
    "However experimenting here to see how models work as realistically want this to be able to work on project by project basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = cv.get_feature_names()\n",
    "tfidf_vectors = transformer.transform(word_count_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Intw 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>difficult</td>\n",
       "      <td>0.401004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>made</td>\n",
       "      <td>0.300753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>kong</td>\n",
       "      <td>0.300753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>know mean</td>\n",
       "      <td>0.300753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>train</td>\n",
       "      <td>0.300753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>hong kong</td>\n",
       "      <td>0.300753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>hong</td>\n",
       "      <td>0.300753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>stories</td>\n",
       "      <td>0.250627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>hes</td>\n",
       "      <td>0.250627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>everyone</td>\n",
       "      <td>0.200502</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Intw 1\n",
       "difficult  0.401004\n",
       "made       0.300753\n",
       "kong       0.300753\n",
       "know mean  0.300753\n",
       "train      0.300753\n",
       "hong kong  0.300753\n",
       "hong       0.300753\n",
       "stories    0.250627\n",
       "hes        0.250627\n",
       "everyone   0.200502"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_doc_vectors= tfidf_vectors[0]\n",
    "first_doc_df = pd.DataFrame(first_doc_vectors.T.todense(), index = cv.get_feature_names(), columns=['Intw 1'])\n",
    "first_doc_df.sort_values(by=['Intw 1'], ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Intw 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>sometimes</td>\n",
       "      <td>0.392306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>fucking</td>\n",
       "      <td>0.343890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>talk</td>\n",
       "      <td>0.326921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>life</td>\n",
       "      <td>0.326921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>everyone</td>\n",
       "      <td>0.261537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>stuff like</td>\n",
       "      <td>0.261537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>friends</td>\n",
       "      <td>0.261537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>couple</td>\n",
       "      <td>0.261537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>like theres</td>\n",
       "      <td>0.196153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>soon</td>\n",
       "      <td>0.196153</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Intw 2\n",
       "sometimes    0.392306\n",
       "fucking      0.343890\n",
       "talk         0.326921\n",
       "life         0.326921\n",
       "everyone     0.261537\n",
       "stuff like   0.261537\n",
       "friends      0.261537\n",
       "couple       0.261537\n",
       "like theres  0.196153\n",
       "soon         0.196153"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sec_doc_vectors= tfidf_vectors[1]\n",
    "sec_doc_df = pd.DataFrame(sec_doc_vectors.T.todense(), index = cv.get_feature_names(), columns=['Intw 2'])\n",
    "sec_doc_df.sort_values(by=['Intw 2'], ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Intw 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>china</td>\n",
       "      <td>0.695059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>stay</td>\n",
       "      <td>0.274471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>school</td>\n",
       "      <td>0.240597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>chinese</td>\n",
       "      <td>0.213477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>journalism</td>\n",
       "      <td>0.160398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>card</td>\n",
       "      <td>0.160398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>green card</td>\n",
       "      <td>0.160398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>friends</td>\n",
       "      <td>0.152484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>american</td>\n",
       "      <td>0.152484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>citizenship</td>\n",
       "      <td>0.147032</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Intw 3\n",
       "china        0.695059\n",
       "stay         0.274471\n",
       "school       0.240597\n",
       "chinese      0.213477\n",
       "journalism   0.160398\n",
       "card         0.160398\n",
       "green card   0.160398\n",
       "friends      0.152484\n",
       "american     0.152484\n",
       "citizenship  0.147032"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "third_doc_vectors= tfidf_vectors[2]\n",
    "third_doc_df = pd.DataFrame(third_doc_vectors.T.todense(), index = cv.get_feature_names(), columns=['Intw 3'])\n",
    "third_doc_df.sort_values(by=['Intw 3'], ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caution on small datasets. \n",
    "\n",
    "However I think this is a good overview of what word count vectors and tfidf can do to give a flavour of what overall themes and each document says. \n",
    "\n",
    "with ngrams the tfidf scores have clearly increased. \n",
    "\n",
    "Additionally feel more confident in creating word vectors and controlling the input/ output by using ngrams, min_df, max_df, appending to stopwords etc. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
